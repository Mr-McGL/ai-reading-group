###################################
# 2025/10
###################################

## Interpretabilidad y gobernanza

Sparse Autoencoders (SAEs) para interpretabilidad de LLMs

* [https://www.goodfire.ai/blog/sae-open-source-announcement](https://www.goodfire.ai/blog/sae-open-source-announcement)
* [https://www.math.inc/gauss](https://www.math.inc/gauss)

Maquinaciones ocultas, modelos que actúan como si estuvieran alineados con el usuario pero no lo están
[https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/](https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/)

Alucinaciones:

* [https://openai.com/es-ES/index/why-language-models-hallucinate/](https://openai.com/es-ES/index/why-language-models-hallucinate/)

Capacidad de razonamiento de los Autoencoders:
[https://arxiv.org/html/2509.02565v2](https://arxiv.org/html/2509.02565v2)

## Agentes

Agente que se usó para la olimpiada de matemáticas:
[https://github.com/lyang36/IMO25](https://github.com/lyang36/IMO25)

Inversión financiera
[https://www.arxiv.org/abs/2509.11420](https://www.arxiv.org/abs/2509.11420)

Deep research:
Tongyi DeepResearch: A New Era of Open-Source AI Researchers
[https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/)

## Razonamiento

RL's Razor: Why Online Reinforcement Learning Forgets Less
[https://arxiv.org/abs/2509.04259](https://arxiv.org/abs/2509.04259)

The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs
[https://arxiv.org/abs/2509.09677](https://arxiv.org/abs/2509.09677)

## Arquitectura

H-Nets:

* [https://main-horse.github.io/hnet/](https://main-horse.github.io/hnet/)

## Modelos

Modelos para matemáticas: Resultados.

* [https://github.com/math-inc/strongpnt](https://github.com/math-inc/strongpnt)

Qwen3-Next y 8B:

* [https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)
* [https://huggingface.co/Qwen/Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B)

Razonamiento eficiente: Ling-Flash: [https://huggingface.co/inclusionAI/Ling-flash-2.0](https://huggingface.co/inclusionAI/Ling-flash-2.0)

###################################

### 2025/09

###################################

## Caja Negra

Why Language Models Hallucinate
[https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf](https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf)
[https://openai.com/index/why-language-models-hallucinate/](https://openai.com/index/why-language-models-hallucinate/)

## Entrenamiento

* µTransfer: A technique for hyperparameter tuning of enormous neural networks
  [https://www.microsoft.com/en-us/research/blog/%C2%B5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/](https://www.microsoft.com/en-us/research/blog/%C2%B5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/)

* Monarch es un framework de programación distribuida:
  [https://github.com/meta-pytorch/monarch](https://github.com/meta-pytorch/monarch)

* Prime Intellect: entrenamiento distribuido.
  [https://www.primeintellect.ai/?_gl=1*3wvwbe*_gcl_au*MTEzMjk0NDQ1OS4xNzU3MzIyNzY1#research](https://www.primeintellect.ai/?_gl=1*3wvwbe*_gcl_au*MTEzMjk0NDQ1OS4xNzU3MzIyNzY1#research)

## Datasets

FineWeb: decanting the web for the finest text data at scale
[https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)

FineVision: Open Data Is All You Need
[https://huggingface.co/spaces/HuggingFaceM4/FineVision](https://huggingface.co/spaces/HuggingFaceM4/FineVision)
