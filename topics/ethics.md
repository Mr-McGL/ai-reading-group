# AI Governance: Ética, Alineamiento, Explicabilidad y Controles de Seguridad en LLMs

## Índice

- [AI Governance: Ética, Alineamiento, Explicabilidad y Controles de Seguridad en LLMs](#ai-governance-ética-alineamiento-explicabilidad-y-controles-de-seguridad-en-llms)
  - [Papers](#papers)

## Papers

* **Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models** (21 feb 2025 – Standford)  
[arXiv](https://arxiv.org/abs/2502.15964)  
**Keywords**: Modelos de lenguaje grandes (*LLMs*), colaboración local-remota, eficiencia de costos, razonamiento sobre datos extensos, seguridad
**Descripción**: Este estudio investiga cómo un modelo de lenguaje pequeño, operando en un dispositivo local con acceso a datos locales, puede colaborar con un modelo de lenguaje avanzado alojado en la nube para abordar tareas del mundo real que implican razonamiento financiero, médico y científico sobre documentos extensos. 

* **Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs** (12 feb 2025 – Center for AI Safety, Berkeley, Pennsylvania)  
  [arXiv](https://arxiv.org/abs/2502.08640)  
  **Keywords**: Valores emergentes, alineación, seguridad en IA  
  **Descripción**: Este estudio investiga la aparición de sistemas de valores coherentes en los LLMs a medida que escalan, y propone una agenda de investigación en la  Ingeniería de Servicios Públicos (*Utility Engineering*), que abarca tanto el análisis como el control de dichas preferencias, detectando comportamientos problemáticos y ofreciendo métodos para alinear las utilidades de la IA con valores humanos.

* **Auditing Prompt Caching in Language Model APIs** (11 feb 2025 – Standford)    
[arXiv](https://arxiv.org/abs/2502.07776)  
**Keywords**: Modelos de lenguaje grandes (*LLMs*), almacenamiento en caché de *prompts*, ataques de canal lateral, privacidad  
**Descripción**: Este estudio investiga cómo el almacenamiento en caché de *prompts* en APIs de modelos de lenguaje grandes puede introducir variaciones en los tiempos de respuesta, lo que podría ser explotado en ataques de canal lateral. Mediante auditorías estadísticas, los autores detectaron que 8 de 17 proveedores de APIs, incluyendo OpenAI, comparten cachés globalmente entre usuarios, lo que podría permitir a un atacante inferir información sobre los *prompts* de otros usuarios basándose en tiempos de respuesta más rápidos. 


