# Agentización

## Índice

- [Agentización](#agentización)
  - [Papers](#papers)
  - [Otros recursos](#otros-recursos)
    - [Código](#código)

## Papers

* **Agentic Reasoning: Reasoning LLMs with Tools for Deep Research** (7 feb 2025 – Oxford)  
  [arXiv](https://arxiv.org/abs/2502.04644) | [GitHub](https://github.com/theworldofagents/Agentic-Reasoning)  
  **Keywords**: *LLMs* (modelos de lenguaje grandes), razonamiento, agentes, herramientas externas, investigación profunda (*deep research*), *knowledge graph* (grafo de conocimiento), RAG
  **Descripción**: Este trabajo introduce **Agentic Reasoning**, un marco que mejora el razonamiento de los *LLMs* integrando *agents* que utilizan herramientas externas.  


* **Agentless: Demystifying LLM-based Software Engineering Agents** (1 jul 2024 – University of Illinois)  
  [arXiv](https://arxiv.org/abs/2407.01489) | [GitHub](https://github.com/OpenAutoCoder/Agentless)  
  **Keywords**: Agentización, modelos de lenguaje grandes (*LLMs*), automatización  
  **Descripción**: Este trabajo presenta un enfoque simplificado para resolver problemas de desarrollo de software sin recurrir a agentes autónomos complejos. A diferencia de métodos anteriores que utilizan agentes capaces de ejecutar comandos y planificar acciones, **Agentless** emplea un proceso de tres fases: localización del problema, reparación y validación del parche. El estudio destaca el potencial de técnicas más simples y rentables en el desarrollo autónomo de software.


* **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena** (28 dec 2023 – UC Berkeley)  
  [OpenReview](https://openreview.net/forum?id=uccHPGDlao) | [GitHub](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge)  
  **Keywords**: Modelos de lenguaje grandes (*LLMs*), preferencia humana, pruebas de evaluación, evaluación  
  **Descripción**: La evaluación de asistentes conversacionales basados en modelos de lenguaje grandes (LLMs) es desafiante debido a la falta de *benchmarks* adecuados para medir preferencias humanas. Se propone el enfoque *LLM-as-a-judge*, donde LLMs avanzados actúan como jueces en evaluaciones abiertas, analizando sesgos y limitaciones. La validación con **MT-bench** y **Chatbot Arena** muestra que modelos como GPT-4 alcanzan más del 80% de concordancia con evaluadores humanos, demostrando que *LLM-as-a-judge* es un método escalable y explicable que complementa los *benchmarks* tradicionales.

## Otros recursos

### Código


* **Agentic Reasoning: Reasoning LLMs with Tools for Deep Research** (7 feb 2025 – Oxford)  
  [GitHub](https://github.com/theworldofagents/Agentic-Reasoning)  
  **Descripción**: Repositorio de **Agentic Reasoning**.


* **Agentless**  (1 jul 2024 – University of Illinois)  
  [GitHub](https://github.com/OpenAutoCoder/Agentless)  
  **Descripción**: Repositorio del proyecto **Agentless**.