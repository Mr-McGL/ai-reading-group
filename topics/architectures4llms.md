# Arquitecturas y Limitaciones de los Grandes Modelos de Lenguaje

## √çndice

- [Arquitecturas y Limitaciones de los Grandes Modelos de Lenguaje](#arquitecturas-y-limitaciones-de-los-grandes-modelos-de-lenguaje)
  - [Papers](#papers)
    - [Arquitecturas](#arquitecturas)
    - [Uso de herramientas](#uso-de-herramientas)
    - [Ventana de Contexto](#ventana-de-contexto)
    - [Eficiencia/Caches](#eficienciacaches)
  - [Otros Recursos](#otros-recursos)
    - [Blogs](#blogs)
    - [Repositorio de art√≠culos](#repositorio-de-art√≠culos)
    - [V√≠deos](#v√≠deos)

## Papers

### Arquitecturas

* **Large Language Diffusion Models** (18 feb 2025 ‚Äì Renmin University of China, Ant Group)  
  [arXiv](https://arxiv.org/pdf/2502.09992)  
  **Keywords**: Modelos de lenguaje grandes, difusi√≥n, generative modeling, in-context learning, razonamiento inverso, escalabilidad  
  **Descripci√≥n**: Este estudio introduce LLaDA, un modelo de difusi√≥n para grandes modelos de lenguaje entrenado desde cero bajo un paradigma de preentrenamiento y fine-tuning supervisado. A diferencia de los modelos autorregresivos tradicionales, LLaDA define la distribuci√≥n del modelo mediante un proceso de enmascaramiento aleatorio y un predictor de m√°scaras basado en Transformers, lo que permite capturar dependencias bidireccionales y superar limitaciones inherentes a la generaci√≥n token a token. Los resultados demuestran que LLaDA es competitivo en escalabilidad y rendimiento en tareas de comprensi√≥n, matem√°ticas, generaci√≥n de c√≥digo y di√°logo, estableciendo a los modelos de difusi√≥n como una alternativa prometedora a los enfoques autorregresivos.


### Uso de herramientas

* **Toolformer: Language Models Can Teach Themselves to Use Tools** (9 feb 2023 ‚Äì META/Pompeu Fabra)  
  [arXiv](https://arxiv.org/abs/2302.04761) | [GitHub](https://github.com/conceptofmind/toolformer)  
  **Keywords**: Modelos de lenguaje grandes (*LLMs*), herramientas externas, aprendizaje auto-supervisado  
  **Descripci√≥n**: Este trabajo introduce **Toolformer**, un modelo que se entrena para decidir qu√© APIs llamar, cu√°ndo llamarlas, qu√© argumentos pasar y c√≥mo incorporar mejor los resultados en la predicci√≥n de tokens futuros. 

### Ventana de Contexto

* **Titans: Learning to Memorize at Test Time** (31 dic 2024 ‚Äì Google Research)  
  [arXiv](https://arxiv.org/abs/2501.00663)  
  **Keywords**: Ventana de contexto, **Long Term Memory** (memoria a largo plazo)  
  **Descripci√≥n**: Este estudio introduce un nuevo m√≥dulo de memoria neural a largo plazo que aprende a memorizar el contexto hist√≥rico y asiste al mecanismo de atenci√≥n para enfocarse en el contexto actual, utilizando informaci√≥n del pasado distante y escalando eficientemente a ventanas de contexto superiores a 2 millones de tokens.

* **LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens** (21 feb 2024 ‚Äì Microsoft Research)  
  [arXiv](https://arxiv.org/abs/2402.13753)  
  **Keywords**: Ventana de contexto, *rotary position embedding*  
  **Descripci√≥n**: Se presenta una innovadora t√©cnica que extiende la ventana de contexto de los LLMs (Large Language Models - Modelos de Lenguaje de Gran Tama√±o) hasta m√°s de 2 millones de tokens mediante estrategias de interpolaci√≥n y extensi√≥n progresiva, manteniendo el rendimiento en tareas con contextos breves.

* **RoFormer: Enhanced Transformer with Rotary Position Embedding** (20 abr 2021 ‚Äì Zhuiyi Technology Co., Ltd. Shenzhen)  
  [arXiv](https://arxiv.org/abs/2104.09864)  
  **Keywords**: Ventana de contexto, *rotary position embedding*  
  **Descripci√≥n**: Propone una mejora en la arquitectura Transformer mediante la incorporaci√≥n de Rotary Position Embedding, lo que permite un modelado m√°s flexible y eficaz de las relaciones posicionales en secuencias, optimizando el desempe√±o en tareas de procesamiento de lenguaje.


### Eficiencia/Caches

* **Auditing Prompt Caching in Language Model APIs** (11 feb 2025 ‚Äì Standford)  
  [arXiv](https://arxiv.org/abs/2502.07776)  
  **Keywords**: Modelos de lenguaje grandes (*LLMs*), almacenamiento en cach√© de *prompts*, ataques de canal lateral, privacidad  
  **Descripci√≥n**: Este estudio investiga c√≥mo el almacenamiento en cach√© de *prompts* en APIs de modelos de lenguaje grandes puede introducir variaciones en los tiempos de respuesta, lo que podr√≠a ser explotado en ataques de canal lateral. Mediante auditor√≠as estad√≠sticas, los autores detectaron que 8 de 17 proveedores de APIs, incluyendo OpenAI, comparten cach√©s globalmente entre usuarios, lo que podr√≠a permitir a un atacante inferir informaci√≥n sobre los *prompts* de otros usuarios bas√°ndose en tiempos de respuesta m√°s r√°pidos. 

* **Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks** (20 dic 2024 ‚Äì Taiwan)  
  [arXiv](https://arxiv.org/abs/2412.15605)  
  **Keywords**: Modelos de lenguaje grandes (*LLMs*), generaci√≥n aumentada por recuperaci√≥n (*Retrieval-Augmented Generation* - RAG), generaci√≥n aumentada por cach√© (*Cache-Augmented Generation* - CAG), eficiencia  
  **Descripci√≥n**: Este trabajo propone la **generaci√≥n aumentada por cach√©** (*Cache-Augmented Generation* - CAG) como una alternativa a la generaci√≥n aumentada por recuperaci√≥n (*Retrieval-Augmented Generation* - RAG) en modelos de lenguaje grandes. CAG implica precargar todos los recursos relevantes en el contexto extendido del modelo antes de la inferencia, eliminando la necesidad de recuperaci√≥n en tiempo real. 


* **Prompt Cache: Modular Attention Reuse for Low-Latency Inference** (7 nov 2023 ‚Äì Google, Yale)  
  [arXiv](https://arxiv.org/abs/2311.04934)  
  **Keywords**: Modelos de lenguaje grandes (*LLMs*), reutilizaci√≥n de estados de atenci√≥n, inferencia de baja latencia  
  **Descripci√≥n**: Este trabajo presenta **Prompt Cache**, una t√©cnica para acelerar la inferencia en modelos de lenguaje grandes mediante la reutilizaci√≥n de estados de atenci√≥n en diferentes *prompts*. 




## Otros Recursos

### Blogs

* ***Mixture of Experts Explained*** (11 dic 2023 ‚Äì Hugging Face)  
  Blog  
  [Hugging Face Blog](https://huggingface.co/blog/moe)  
  **Descripci√≥n**: Entrada del blog que explica en detalle el concepto de Mixture of Experts (MoE), sus fundamentos, ventajas, desaf√≠os y aplicaciones en modelos de lenguaje y transformers, con √©nfasis en t√©cnicas de entrenamiento y fine-tuning para modelos dispersos.

* ***How Scaling Laws Drive Smarter, More Powerful AI*** (12 feb 2025 ‚Äì NVIDIA)  
  Blog  
  [Blog de NVIDIA](https://blogs.nvidia.com/blog/ai-scaling-laws/#:~:text=Scaling%20laws%20describe%20how%20the,parameters%20or%20computational%20resources%20increases.)  
  **Descripci√≥n**: Entrada del blog que detalla c√≥mo las leyes de escalabilidad en IA establecen la relaci√≥n entre la cantidad de datos, par√°metros y recursos computacionales con la mejora en el rendimiento de los modelos. Explica conceptos de preentrenamiento, postentrenamiento y escalado en tiempo de inferencia, poniendo especial √©nfasis en la importancia de aplicar computaci√≥n acelerada para soportar modelos de razonamiento complejo.


* ***Automating GPU Kernel Generation with DeepSeek R1 and Inference Time Scaling*** (fecha ‚Äì NVIDIA Developer)  
  Blog  
  [Developer Blog de NVIDIA](https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/)  
  **Descripci√≥n**: Entrada del blog que explica c√≥mo DeepSeek R1 automatiza la generaci√≥n de kernels para GPU, permitiendo optimizar el rendimiento en tiempo de inferencia. El art√≠culo aborda t√©cnicas avanzadas de deep learning para la generaci√≥n eficiente de c√≥digo en GPU y describe c√≥mo el escalado en tiempo de inferencia puede mejorar la eficiencia y capacidad de respuesta de los modelos de inteligencia artificial en producci√≥n.


* üî•üî•üî•***Data-optimal scaling laws*** (2025 ‚Äì Life Architect)  
  Blog  
  [Blog](https://lifearchitect.ai/chinchilla/)  
  **Descripci√≥n**: Resumen del escaladado de datos (ratio datos/parametros para maximizar un coste computacional dado)

### Repositorio de art√≠culos

  * ***Awesome LLM*** (2025 ‚Äì autor: Hannibal046)  
    Repositorio de art√≠culos 
    [GitHub](https://github.com/Hannibal046/Awesome-LLM)  
    **Descripci√≥n**: Repositorio que recopila una amplia variedad de recursos, herramientas y proyectos relacionados con los modelos de lenguaje grandes (LLMs).


### V√≠deos

* **RoPE Rotary Position Embedding to 100K Context Length** (23 may 2024 ‚Äì DiscoverAI)  
  [YouTube](https://www.youtube.com/watch?v=DvP8f7eWS7U)  
  **Descripci√≥n**: An√°lisis de RoPE aplicado a la extensi√≥n de la ventana de contexto.

* **LongRoPE & Theta Scaling to 1 Mio Token** (25 may 2024 ‚Äì DiscoverAI)  
  [YouTube](https://www.youtube.com/watch?v=RCSvpYb90qE)  
  **Descripci√≥n**: An√°lisis de LongRoPE en la ampliaci√≥n de la ventana de contexto.




