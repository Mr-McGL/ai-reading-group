# Arquitecturas y Limitaciones de los Grandes Modelos de Lenguaje

## Índice

- [Arquitecturas y Limitaciones de los Grandes Modelos de Lenguaje](#arquitecturas-y-limitaciones-de-los-grandes-modelos-de-lenguaje)
  - [Papers](#papers)
    - [Ventana de Contexto](#ventana-de-contexto)
  - [Otros Recursos](#otros-recursos)
    - [Vídeos](#vídeos)

## Papers

### Ventana de Contexto

* **Titans: Learning to Memorize at Test Time** (31 dic 2024 – Google Research)  
  [arXiv](https://arxiv.org/abs/2501.00663)  
  **Keywords**: Ventana de contexto, **Long Term Memory** (memoria a largo plazo)  
  **Descripción**: Este estudio introduce un nuevo módulo de memoria neural a largo plazo que aprende a memorizar el contexto histórico y asiste al mecanismo de atención para enfocarse en el contexto actual, utilizando información del pasado distante y escalando eficientemente a ventanas de contexto superiores a 2 millones de tokens.

* **LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens** (21 feb 2024 – Microsoft Research)  
  [arXiv](https://arxiv.org/abs/2402.13753)  
  **Keywords**: Ventana de contexto, *rotary position embedding*  
  **Descripción**: Se presenta una innovadora técnica que extiende la ventana de contexto de los LLMs (Large Language Models - Modelos de Lenguaje de Gran Tamaño) hasta más de 2 millones de tokens mediante estrategias de interpolación y extensión progresiva, manteniendo el rendimiento en tareas con contextos breves.

* **RoFormer: Enhanced Transformer with Rotary Position Embedding** (20 abr 2021 – Zhuiyi Technology Co., Ltd. Shenzhen)  
  [arXiv](https://arxiv.org/abs/2104.09864)  
  **Keywords**: Ventana de contexto, *rotary position embedding*  
  **Descripción**: Propone una mejora en la arquitectura Transformer mediante la incorporación de Rotary Position Embedding, lo que permite un modelado más flexible y eficaz de las relaciones posicionales en secuencias, optimizando el desempeño en tareas de procesamiento de lenguaje.

## Otros Recursos

### Vídeos

* **RoPE Rotary Position Embedding to 100K Context Length** (23 may 2024 – DiscoverAI)  
  [YouTube](https://www.youtube.com/watch?v=DvP8f7eWS7U)  
  **Descripción**: Análisis de RoPE aplicado a la extensión de la ventana de contexto.

* **LongRoPE & Theta Scaling to 1 Mio Token** (25 may 2024 – DiscoverAI)  
  [YouTube](https://www.youtube.com/watch?v=RCSvpYb90qE)  
  **Descripción**: Análisis de LongRoPE en la ampliación de la ventana de contexto.




