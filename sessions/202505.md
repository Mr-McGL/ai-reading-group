# Reading Group (mayo)

## 06/05/2025 - Multimodalidad

**Image-to-LaTeX Converter for Mathematical Formulas and Text** (7 ago 2024 ‚Äì Saarland University)  
[arXiv](https://arxiv.org/abs/2408.04015)  | [GitHub](https://github.com/d-gurgurov/im2latex?tab=readme-ov-file)  

<details markdown="1">

<summary><strong>Resumen:</strong></summary>


**Keywords**: Multimodalidad, Imagen, Texto, OCR, *transformers*, LoRA  

**Descripci√≥n**: En este proyecto se entrena un modelo encoder-decoder de visi√≥n para generar c√≥digo LaTeX a partir de im√°genes que contienen f√≥rmulas matem√°ticas y texto. Se desarrollan dos versiones: una base, que utiliza un encoder basado en Swin Transformer y un decodificador basado en GPT-2 entrenado con im√°genes generadas autom√°ticamente, y otra afinada mediante Low-Rank Adaptation (LoRA) entrenada con f√≥rmulas manuscritas. Se eval√∫a la calidad de la conversi√≥n mediante la m√©trica BLEU y se comparan los resultados con modelos similares, como Pix2Text, TexTeller y Sumen. El proyecto aporta modelos de c√≥digo abierto y c√≥digo desde cero para la construcci√≥n de estos sistemas con entrenamiento distribuido y optimizaci√≥n en GPU.

</details>

<!--<details markdown="1">
  <summary><strong>Referencias relevantes:</strong></summary>
</details>-->

<details markdown="1">

<summary><strong>Recursos:</strong></summary>

* üî•***Awesome Multimodal LLMs*** (2024 ‚Äì BradyFU)  
  Repositorio de art√≠culos  
  [GitHub](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)  
  **Descripci√≥n**: Repositorio que recopila y organiza recursos, herramientas y proyectos relacionados con modelos de lenguaje grandes multimodales, facilitando el acceso a informaci√≥n y ejemplos pr√°cticos sobre esta tem√°tica.

* ***Vision Parser:*** (enero 2025 ‚Äì GitHub) .  
  [git](https://github.com/iamarunbrahma/vision-parse)
  **Descripci√≥n**: üî•üî•üî• Acepta m√∫ltiples modelos. Basado en Ollama
  
* üî•üî•üî• ***AnyModel*** (2025) 
  [git](https://github.com/ritabratamaiti/AnyModal), [HF](https://huggingface.co/AnyModal/LaTeX-OCR-Llama-3.2-1B),  [dataset 1](https://huggingface.co/datasets/unsloth/LaTeX_OCR), [dataset 2](https://huggingface.co/datasets/linxy/LaTeX_OCR)  
  **Descripci√≥n**: Encoder with Llama 3.2-1B. üî•üî•üî• AnyModal es un framework modular y extensible para integrar diversas modalidades de entrada.

* ***MinerU*** (2025) 
  [git](https://github.com/opendatalab/MinerU), [doc](https://mineru.readthedocs.io/en/latest/index.html), [api](https://mineru.readthedocs.io/en/latest/user_guide/usage/api.html)  
  AGPL
  **Descripci√≥n**: Converts PDFs into machine-readable formats

* **im2latex** (7 ago 2024 ‚Äì GitHub)  
  [GitHub](https://github.com/d-gurgurov/im2latex?tab=readme-ov-file)  
  **Descripci√≥n**: Repositorio que contiene c√≥digo y documentaci√≥n para un convertidor de im√°genes a LaTeX, facilitando la transformaci√≥n de f√≥rmulas matem√°ticas y texto a c√≥digo LaTeX mediante t√©cnicas de visi√≥n por ordenador.

* **TexTeller** (6 jun 2024 ‚Äì GitHub)  
  [GitHub](https://github.com/OleehyO/TexTeller)  
  **Descripci√≥n**: Repositorio que ofrece un sistema de reconocimiento de f√≥rmulas en im√°genes, permitiendo convertir im√°genes a c√≥digo LaTeX con alta precisi√≥n y robustez, basado en modelos end-to-end y t√©cnicas avanzadas de OCR.

* ‚ö†Ô∏è**Pix2Text** (15 jul 2024 ‚Äì GitHub) ‚≠ê‚≠ê‚≠ê  
  [GitHub](https://github.com/breezedeus/Pix2Text)  
  **Descripci√≥n**: Repositorio que proporciona un modelo para convertir im√°genes en texto. Basado en t√©cnicas de OCR y aprendizaje profundo, destaca por su capacidad de generalizaci√≥n y precisi√≥n en la extracci√≥n de contenido visual.

</details>


<details markdown="1">
  <summary><strong>Otros <emph>papers</emph> (ideas para pr√≥ximos grupos de lectura)/recursos:</strong></summary>

  ***Papers:***

  * **MinerU: An Open-Source Solution for Precise Document Content Extraction** (sep 2024 ‚Äì Shanghai Artificial Intelligence Laboratory)  
  [arXiv](https://arxiv.org/abs/2409.18839)  
  **Keywords**: extracci√≥n de contenido, documentos, c√≥digo abierto, visi√≥n por computador, RAG, multimodalidad
  **Descripci√≥n**: Este art√≠culo presenta MinerU, una soluci√≥n de c√≥digo abierto para la extracci√≥n precisa de contenido en documentos. Los autores proponen un m√©todo innovador que integra t√©cnicas de procesamiento de imagen y algoritmos de reconocimiento para identificar y extraer informaci√≥n relevante, valid√°ndolo mediante experimentos comparativos.

  * **Reasoning Models Don't Always Say What They Think** (3 abr 2025 ‚Äì Anthropic)  
  [Anthropic](https://www.anthropic.com/research/reasoning-models-dont-say-think)  
  **Keywords**: Modelos de lenguaje, razonamiento, explicaciones no fieles  
  **Descripci√≥n**: Este estudio examina la fidelidad de las explicaciones generadas por modelos de lenguaje en procesos de razonamiento. Se encontr√≥ que, aunque los modelos utilizan pistas proporcionadas en las preguntas para formular sus respuestas, a menudo omiten mencionar estas pistas en sus explicaciones, lo que plantea preocupaciones sobre la transparencia y confiabilidad de sus procesos de razonamiento.

  * **PaperBench: Evaluating AI‚Äôs Abilities to Replicate AI Research** (2025 ‚Äì OpenAI)  
  [OpenAI](https://cdn.openai.com/papers/22265bac-3191-44e5-b057-7aaacd8e90cd/paperbench.pdf)  
  **Keywords**: replicaci√≥n, benchmark, inteligencia artificial, evaluaci√≥n, r√∫bricas  
  **Descripci√≥n**: Este art√≠culo presenta PaperBench, un benchmark dise√±ado para evaluar la capacidad de agentes de IA para replicar investigaciones en el √°mbito de la inteligencia artificial. Los agentes deben replicar por completo los experimentos descritos en 20 art√≠culos presentados en ICML 2024, partiendo de cero y sin utilizar el c√≥digo original. La evaluaci√≥n se fundamenta en r√∫bricas detalladas, co-dise√±adas con los autores de los trabajos, que desglosan cada proceso de replicaci√≥n en m√∫ltiples sub-tareas (desde la implementaci√≥n del c√≥digo hasta la ejecuci√≥n y verificaci√≥n de resultados). Adem√°s, se introduce un evaluador autom√°tico basado en LLM para calificar escalablemente cada intento de replicaci√≥n.  
  **Replica de google**: 
  [AI co-scientist](https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/)  
  [Resultados](https://www.forbes.com/sites/lesliekatz/2025/02/19/google-unveils-ai-co-scientist-to-supercharge-research-breakthroughs/)  
  **Sarkana**:  
  [The AI Scientist Generates its First Peer-Reviewed Scientific Publication](https://sakana.ai/ai-scientist-first-publication/)

  * **Neuroscience-Inspired Artificial Intelligence** (19 jul 2017 ‚Äì Deepmind)  
  [Neuron](https://doi.org/10.1016/j.neuron.2017.06.011)  
  **Keywords**: Inteligencia artificial, neurociencia, modelos inspirados en el cerebro  
  **Descripci√≥n**: Este art√≠culo argumenta que una mejor comprensi√≥n de los cerebros biol√≥gicos puede desempe√±ar un papel vital en la construcci√≥n de m√°quinas inteligentes. Se examinan las interacciones hist√≥ricas entre los campos de la inteligencia artificial y la neurociencia, destacando avances actuales en IA inspirados en el estudio de la computaci√≥n neuronal en humanos y otros animales. Se concluye resaltando temas compartidos que pueden ser clave para avanzar en futuras investigaciones en ambos campos.

  * **LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models** (14 nov 2024 ‚Äì NVIDIA)  
  [arXiv](https://arxiv.org/abs/2411.09595)  
  **Keywords**: Modelos de lenguaje grandes (*LLMs*), generaci√≥n de mallas 3D, integraci√≥n de modalidades, conocimiento espacial, tokenizaci√≥n de mallas  
  **Descripci√≥n**: Este estudio explora la ampliaci√≥n de las capacidades de los *LLMs* entrenados en texto para generar mallas 3D. Se propone representar las coordenadas de los v√©rtices y las definiciones de las caras como texto, lo que permite la integraci√≥n directa sin necesidad de expandir el vocabulario. Se construye un conjunto de datos para *fine-tuning* supervisado que habilita la generaci√≥n de mallas a partir de indicaciones textuales, la producci√≥n de salidas intercaladas (texto y mallas) y la comprensi√≥n de las mallas. Con este enfoque, LLaMA-Mesh alcanza una calidad comparable a la de modelos entrenados desde cero sin afectar el rendimiento en la generaci√≥n textual.

  * >>>>>> **Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems** (31 mar 2025 ‚Äì Google, Meta ...)  
  [arXiv](https://www.arxiv.org/abs/2504.01990) | [github_papers](https://github.com/FoundationAgents/awesome-foundation-agents?tab=readme-ov-file)
  **Keywords**: Modelos de lenguaje grandes (*LLMs*), agentes inteligentes, auto-mejora, evoluci√≥n colaborativa, seguridad  
  **Descripci√≥n**: Este art√≠culo ofrece una revisi√≥n exhaustiva sobre los fundamentos y desaf√≠os en el desarrollo de agentes inteligentes basados en modelos de lenguaje grandes. Se explora una arquitectura modular inspirada en el cerebro humano que integra componentes para la percepci√≥n, memoria, modelado del mundo, procesamiento de recompensas y sistemas an√°logos a las emociones. Adem√°s, se analizan mecanismos de auto-mejora y evoluci√≥n adaptativa, as√≠ como la colaboraci√≥n en sistemas multiagente, subrayando la necesidad de construir sistemas seguros, √©ticos y robustos para su implementaci√≥n en entornos reales.

  ***Recursos:***

  * üî•***LLM Reasoning Papers*** (15 ene 2025 ‚Äì philschmid)  
    Repositorio de art√≠culos  
    [Hugging Face](https://huggingface.co/collections/philschmid/llm-reasoning-papers-66e6abbdf5579b829f214de8)  
    **Descripci√≥n**: Colecci√≥n curada que re√∫ne art√≠culos para mejorar las capacidades de razonamiento de los modelos de lenguaje grandes.  

  * ***Awesome LLM*** (2025 ‚Äì Hannibal046)  
    Repositorio de art√≠culos 
    [GitHub](https://github.com/Hannibal046/Awesome-LLM)  
    **Descripci√≥n**: Repositorio que recopila una amplia variedad de recursos, herramientas y proyectos relacionados con los modelos de lenguaje grandes (LLMs).

  * ***Mixture of Experts Explained*** (11 dic 2023 ‚Äì Hugging Face)  
    Blog  
    [Hugging Face Blog](https://huggingface.co/blog/moe)  
    **Descripci√≥n**: Entrada del blog que explica en detalle el concepto de Mixture of Experts (MoE), sus fundamentos, ventajas, desaf√≠os y aplicaciones en modelos de lenguaje y transformers, con √©nfasis en t√©cnicas de entrenamiento y fine-tuning para modelos dispersos.

  * ***How Scaling Laws Drive Smarter, More Powerful AI*** (12 feb 2025 ‚Äì NVIDIA)  
    Blog  
    [Blog de NVIDIA](https://blogs.nvidia.com/blog/ai-scaling-laws/#:~:text=Scaling%20laws%20describe%20how%20the,parameters%20or%20computational%20resources%20increases.)  
    **Descripci√≥n**: Entrada del blog que detalla c√≥mo las leyes de escalabilidad en IA establecen la relaci√≥n entre la cantidad de datos, par√°metros y recursos computacionales con la mejora en el rendimiento de los modelos. Explica conceptos de preentrenamiento, postentrenamiento y escalado en tiempo de inferencia, poniendo especial √©nfasis en la importancia de aplicar computaci√≥n acelerada para soportar modelos de razonamiento complejo.
  
  * ***Automating GPU Kernel Generation with DeepSeek R1 and Inference Time Scaling*** (fecha ‚Äì NVIDIA Developer)  
    Blog  
    [Developer Blog de NVIDIA](https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/)  
    **Descripci√≥n**: Entrada del blog que explica c√≥mo DeepSeek R1 automatiza la generaci√≥n de kernels para GPU, permitiendo optimizar el rendimiento en tiempo de inferencia. El art√≠culo aborda t√©cnicas avanzadas de deep learning para la generaci√≥n eficiente de c√≥digo en GPU y describe c√≥mo el escalado en tiempo de inferencia puede mejorar la eficiencia y capacidad de respuesta de los modelos de inteligencia artificial en producci√≥n.

  * ***NVIDIA: Cursos RAG + Agentes*** (NVIDIA)  
    Curso  
    [Buscar Cursos](https://www.nvidia.com/en-us/training/find-training/) | [RAG 1](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-15+V1) | [RAG 2](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-16+V1)  
    **Descripci√≥n**: Agentes + RAG + Cursos varios
  
  * üî•üî•üî•***Data-optimal scaling laws*** (2025 ‚Äì Life Architect)  
    Blog  
    [Blog](https://lifearchitect.ai/chinchilla/)  
    **Descripci√≥n**: Resumen del escaladado de datos (ratio datos/parametros para maximizar un coste computacional dado)

</details>


