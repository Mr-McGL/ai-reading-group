# Reading Group (Febrero)

## 05/02/2024 - Razonamiento

**The Surprising Effectiveness of Test-Time Training for Abstract Reasoning** (11 nov 2024 - MIT)  
https://arxiv.org/pdf/2411.07279  

  <details>
  <summary><strong>Resumen: </strong></summary>
  <br>

  *Key points*:

  * TTT - Test Time Training. Ajuste temporal de parámetros del modelo en tiempo de inferencia.
  * La construcción del juego de datos de entrenamiento es muy espécifico para el problema tratado ARC
  * Estudio de ablación sólido

  <br>

  Este estudio investiga la eficacia del entrenamiento en tiempo de prueba (TTT) para mejorar las capacidades de razonamiento de los modelos de lenguaje, utilizando el *Abstraction and Reasoning Corpus* (ARC) como referencia. Mediante experimentos sistemáticos, identifican tres componentes cruciales para el éxito del TTT basado en *in-context learning* (dar a la LLM una lista de entradas y salidas, junto con la entrada que debe resolver): (1) ajuste fino inicial en tareas similares, (2) estrategia de entrenamiento (creación del *dataset* de entrenamiento y función de pérdida) y (3) modelo de inferencia (modelo de inferencia aumentada —ojo con *greedy decoding*, usan transformaciones—, esquemas de votación). Por último, añaden su TTT a un modelo inductivo (BARC), mejorando significativamente su rendimiento.
  
  </details>

  <details>
  <summary><strong>Referencias relevantes: </strong></summary>
    
  <br>
  
  * **Combining Induction and Transduction for Abstract Reasoning** (4 nov 2024 - Cornell, Autodesk)  
    https://arxiv.org/abs/2411.02272  
    *Notas*: ste estudio investiga si, al aprender una correspondencia entrada-salida a partir de muy pocos ejemplos, es mejor inferir primero una función latente que explique los ejemplos o predecir directamente nuevas salidas de prueba.e entrena en variaciones sintéticas de programas en Python que resuelven tareas de ARC.e encuentra que los modelos inductivos y transductivos resuelven diferentes tipos de problemas de prueba, y que combinarlos se aproxima al rendimiento humano en ARC.
  * **Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation** (10 abr 2024 - ETH Zurich)  
    https://arxiv.org/abs/2404.07353  
    *Notas*: ste trabajo presenta un código para generar procedimentalmente ejemplos para las tareas de entrenamiento de ARC.ara cada una de las 400 tareas, se creó un generador de ejemplos que sigue la lógica de transformación de los ejemplos originales.sto permite realizar una amplia gama de experimentos que pueden ser pasos importantes hacia avances en el benchmark.

  </details>

  <details>
  <summary><strong>Otros papers:</strong></summary>

  <br>

  * **STaR: Bootstrapping Reasoning With Reasoning** (20 may 2022 - Google Research)  
    https://arxiv.org/abs/2203.14465  
    *Notas*: Primeros intentos de razonamiento.  
  * **Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective** (18 dic 2024 - Shanghai AI Laboratory)  
    https://arxiv.org/abs/2412.14135  
    *Notas*: Supuesta arquitectura de o1.  
  * **DeepSeek-R1** (19 feb 2025 - DeepSeek)  
    https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf  
    *Notas*: Modelo *open source*.  
  * **DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models** (5 feb 2024 - DeepSeek)  
    https://arxiv.org/abs/2402.03300  
    *Notas*: ste trabajo presenta DeepSeekMath 7B, un modelo que amplía las capacidades de DeepSeek-Coder-Base-v1.5 7B mediante un preentrenamiento adicional con 120.000 millones de tokens relacionados con matemáticas.l modelo logra una puntuación del 51,7% en el benchmark MATH sin depender de herramientas externas ni técnicas de votación, acercándose al rendimiento de modelos como Gemini-Ultra y GPT-4.demás, _***se desarrolla el modelo de aprendizaje por refuerzo (RL) utilizado en R1***_
  * **Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters** (22 ene 2025 - Google y UC Berkeley)  
    https://openreview.net/forum?id=4FWAwZtd2n  
    *Notas*: Este estudio de Google y UC Berkeley analiza cómo la asignación óptima de recursos computacionales durante la inferencia puede superar a modelos mucho más grandes en evaluaciones equivalentes en FLOPs. 
  * **Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps** (enero 2025 - DeepMind)  
    https://arxiv.org/abs/2501.09732  
    *Notas*: Este trabajo investiga cómo el rendimiento de los modelos de difusión puede mejorar con un aumento en el cómputo durante la inferencia, más allá de simplemente incrementar los pasos de denoising. 
  * **The Lessons of Developing Process Reward Models in Mathematical Reasoning** (enero 2025 - QWEN)  
    https://arxiv.org/abs/2501.07301  
    *Notas*: El equipo QWEN presenta prácticas y lecciones en la construcción de modelos de recompensa de procesos para el razonamiento matemático, destacando desafíos en la anotación de datos y metodologías de evaluación. 

</details>

## 19/02/2024 - Razonamiento II

**Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters** (22 ene 2025 - Google y UC Berkeley)  
https://openreview.net/forum?id=4FWAwZtd2n  
https://arxiv.org/abs/2408.03314  
*Notas*: Este estudio de Google y UC Berkeley analiza cómo la asignación óptima de recursos computacionales durante la inferencia puede superar a modelos mucho más grandes en evaluaciones equivalentes en FLOPs. 


  <details>  
  <summary><strong>Resumen: </strong></summary>  
  <br>  

  *Key points*:  

  * PRM: Process Review Models  
  * ORM: Output Review Model  
  * Modelos de revisión  
  * Estimación de la dificultad del problema  

  <br>  

  El artículo analiza cómo el uso de un mayor tiempo de cómputo durante la inferencia en modelos grandes de lenguaje (LLM) puede mejorar su rendimiento en tareas difíciles. Los autores investigan dos mecanismos principales para escalar el cómputo en tiempo de prueba:  

  1. **Búsqueda guiada por modelos de recompensa verificadores basados en procesos densos**: este enfoque implica generar múltiples respuestas y evaluarlas mediante un modelo verificador para seleccionar la más adecuada.  
  2. **Actualización adaptativa de la distribución de respuestas del modelo**: en este caso, el modelo ajusta dinámicamente sus respuestas.  

  </details>


  <details>
  <summary><strong>Otros papers</strong></summary>

  * **Competitive Programming with Large Reasoning Models** (febrero 2025 - OpenAI)  
    https://arxiv.org/abs/2502.06807  
    *Notas*: Este estudio demuestra que el uso de aprendizaje por refuerzo en modelos de lenguaje de gran tamaño mejora significativamente el rendimiento en tareas complejas de programación y razonamiento. .

  * **Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach** (febrero 2025 - Max Plank Intitute, Universidad de Maryland y Lawrence Livermore National Laboratory)  
    https://arxiv.org/abs/2502.05171  
    *Notas*: Arquitectura que escala el cómputo en tiempo de prueba mediante razonamiento implícito en el espacio latente. 

  * **Transformer²: Self-Adaptive LLMs** (enero 2025 - Sakana AI)  
    https://sakana.ai/transformer-squared/  
    *Notas*: Sistema de aprendizaje automático que ajusta dinámicamente sus pesos para adaptarse a diversas tareas en tiempo real. Utilizando **descomposición en valores singulares y aprendizaje por refuerzo**, este enfoque permite que los modelos de lenguaje se adapten a nuevas tareas sin necesidad de reentrenamiento extenso, mejorando la eficiencia y el rendimiento en tareas específicas. 


* **Titans: Learning to Memorize at Test Time** (diciembre 2024 - Google Research)  
  https://arxiv.org/abs/2501.00663  
  *Notas*: Este estudio introduce un nuevo módulo de memoria neural a largo plazo que aprende a memorizar el contexto histórico y asiste al mecanismo de atención para enfocarse en el contexto actual, utilizando información del pasado distante, escalando eficientemente a ventanas de contexto mayores a 2 millones. 

  </details>

  <details>
  <summary><strong>Otros recursos</strong></summary>

  <br>

  Aprendizage por refuerzo:
  * **DeepScaleR-1.5B-Preview** (febrero 2025 - Agentica)  
  https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview  
  *Notas*: DeepScaleR-1.5B-Preview es un modelo de lenguaje ajustado a partir de DeepSeek-R1-Distilled-Qwen-1.5B utilizando aprendizaje por refuerzo distribuido.

  * **TinyZero: Reproducción de DeepSeek R1-Zero** (febrero 2025 - Jiayi Pan -  Berkeley)  
  https://github.com/Jiayi-Pan/TinyZero  
  *Notas*: TinyZero es una implementación accesible y minimalista de DeepSeek R1-Zero, enfocada en tareas de cuenta regresiva y multiplicación. 

  * **R1-V: Reforzando la Capacidad de Generalización en Modelos Visión-Lenguaje con Menos de $3** (febrero 2025 - Deep-Agent)  
  https://github.com/Deep-Agent/R1-V  
  *Notas*: R1-V demuestra que el aprendizaje por refuerzo con recompensas verificables supera al ajuste supervisado tradicional en modelos visión-lenguaje. 

  * **Reinforcement Fine-Tuning—12 Days of OpenAI: Day 2** (diciembre 2024 - OpenAI)  
  https://www.youtube.com/watch?v=yCIYS9fx56U  
  https://openai.com/form/rft-research-program/
  *Notas*: Miembros del equipo de OpenAI presentan el programa de investigación sobre ajuste fino mediante refuerzo, destacando su importancia en el desarrollo de modelos de lenguaje más precisos y eficientes.

  </details>  





